{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([torch.randn(2, 2, requires_grad=True)], lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "class MultiStepwithDoubleLinearWarmup(LRScheduler):\n",
    "    def __init__(self, optimizer, milestones=[], gamma=1e-1,eta_max=None, eta_medium=0.0, eta_min=0.0, warmup_iters2=0, inter_warmups_iters=0, warmup_iters1=0, last_epoch=-1,\n",
    "                 verbose=False):\n",
    "        assert eta_max >= eta_medium >= eta_min >= 0.0, 'sa'\n",
    "        self.milestones = Counter(milestones)\n",
    "        self.gamma = gamma\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_medium = eta_medium\n",
    "        self.eta_min = eta_min\n",
    "        self.warmup_iters2 = warmup_iters2\n",
    "        self.inter_warmups_iters = inter_warmups_iters\n",
    "        self.warmup_iters1 = warmup_iters1\n",
    "        if eta_min > 0.0:\n",
    "            for groups in optimizer.param_groups:\n",
    "                groups['lr'] = eta_min\n",
    "        elif eta_medium > 0.0:\n",
    "            for groups in optimizer.param_groups:\n",
    "                groups['lr'] = eta_medium\n",
    "        elif eta_max == 0.0:\n",
    "            raise ValueError('eta_max must be greater than 0.0')\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0 or self.warmup_iters1 < self.last_epoch <= self.warmup_iters1 + self.inter_warmups_iters:\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "        if self.last_epoch <= self.warmup_iters1:\n",
    "            return [self.eta_min + (self.eta_medium - self.eta_min) * self.last_epoch / self.warmup_iters1\n",
    "                    for _ in self.optimizer.param_groups]\n",
    "        \n",
    "        if self.last_epoch <= self.warmup_iters1 + self.inter_warmups_iters + self.warmup_iters2:\n",
    "            return [self.eta_medium + (self.eta_max - self.eta_medium) * (self.last_epoch-(self.warmup_iters1 + self.inter_warmups_iters)) / self.warmup_iters2\n",
    "                    for _ in self.optimizer.param_groups]\n",
    "        if self.last_epoch not in self.milestones:\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "        return [group['lr'] * self.gamma ** self.milestones[self.last_epoch]\n",
    "                for group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([torch.randn(2, 2, requires_grad=True)], lr=0.1)\n",
    "scheduler = MultiStepwithDoubleLinearWarmup(optimizer, [], 1e-1, 0.5, 0.3, 0.1, 10, 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1]\n",
      "[0.14]\n",
      "[0.18]\n",
      "[0.22]\n",
      "[0.26]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.32]\n",
      "[0.33999999999999997]\n",
      "[0.36]\n",
      "[0.38]\n",
      "[0.4]\n",
      "[0.42000000000000004]\n",
      "[0.44]\n",
      "[0.45999999999999996]\n",
      "[0.48]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n",
      "[0.5]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(150):\n",
    "    print(scheduler.get_last_lr())\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
